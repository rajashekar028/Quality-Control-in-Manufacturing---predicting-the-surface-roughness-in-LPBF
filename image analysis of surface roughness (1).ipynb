{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869938fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6ee7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf5d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/giacomomarchioro/pyx3p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9c9254",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'STV1 Rib 1 Surf 1 subset-t.x3p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mx3p\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m X3Pfile\n\u001b[0;32m----> 2\u001b[0m anx3pfile \u001b[38;5;241m=\u001b[39m X3Pfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTV1 Rib 1 Surf 1 subset-t.x3p\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# access an attribute\u001b[39;00m\n\u001b[1;32m      4\u001b[0m anx3pfile\u001b[38;5;241m.\u001b[39mrecord1\u001b[38;5;241m.\u001b[39mfeaturetype\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/x3p/x3p.py:47\u001b[0m, in \u001b[0;36mX3Pfile.__init__\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging \u001b[38;5;241m=\u001b[39m logging\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(filepath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/x3p/x3p.py:113\u001b[0m, in \u001b[0;36mX3Pfile.load\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# The x3p file format is zipped.\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     zfile \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZipFile(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# We read the md5 checksum from the file inside the .zip\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Note: there is also the *main.xml we use `.split` to eliminate it.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# We use as convention to convert checksum to lower case letters.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     checksum_line \u001b[38;5;241m=\u001b[39m zfile\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmd5checksum.hex\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'STV1 Rib 1 Surf 1 subset-t.x3p'"
     ]
    }
   ],
   "source": [
    "from x3p import X3Pfile\n",
    "anx3pfile = X3Pfile('STV1 Rib 1 Surf 1 subset-t.x3p')\n",
    "# access an attribute\n",
    "anx3pfile.record1.featuretype\n",
    "anx3pfile.record1.axes.CX.axistype\n",
    "# access the data\n",
    "anx3pfile.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72229a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'anx3pfile.data' gives you a NumPy array\n",
    "data_array = np.array(anx3pfile.data)\n",
    "\n",
    "# Basic statistical analysis\n",
    "mean = np.mean(data_array)\n",
    "std_dev = np.std(data_array)\n",
    "\n",
    "print(f\"Mean: {mean}, Standard Deviation: {std_dev}\")\n",
    "\n",
    "# Simple 2D visualization\n",
    "plt.imshow(data_array, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Surface Data Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56554b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea021f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('data_array.txt', data_array, fmt='%.5f')  # Adjust the format as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print(data_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10391e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbc669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.DataFrame(data_array)\n",
    "# pd.set_option('display.max_rows', None)  # Display all rows\n",
    "# pd.set_option('display.max_columns', None)  # Display all columns\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(data_array).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba954e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.nanmean(data_array)\n",
    "print(f\"Mean: {mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace nan values with the mean of the non-nan values\n",
    "data_filled = np.nan_to_num(data_array, nan=np.nanmean(data_array))\n",
    "\n",
    "# Calculate mean and standard deviation of the filled data\n",
    "mean_filled = np.mean(data_filled)\n",
    "std_dev_filled = np.std(data_filled)\n",
    "\n",
    "print(f\"Mean (with nan replaced): {mean_filled}, Standard Deviation (with nan replaced): {std_dev_filled}\")\n",
    "\n",
    "# Visualization with nan values replaced\n",
    "plt.imshow(data_filled, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Surface Data Visualization with NaN Values Replaced')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.isnan(data_filled).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data_array' is your NumPy array from the X3P file\n",
    "\n",
    "# Remove any rows and columns that contain nan values\n",
    "data_no_nan = data_array[~np.isnan(data_array).any(axis=1), :]\n",
    "data_no_nan = data_no_nan[:, ~np.isnan(data_no_nan).any(axis=0)]\n",
    "\n",
    "# Visualization\n",
    "plt.imshow(data_no_nan, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Surface Data Visualization with NaN Rows/Columns Removed')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fd6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c8f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86361495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9a9c3c",
   "metadata": {},
   "source": [
    "# comparing surf 7 and surf 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 7 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj7 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj7.data.shape) > 2:\n",
    "    axest = x3pobj7.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj7.data.shape[0]):\n",
    "            plt.imshow(x3pobj7.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj7.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj7.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466634e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj7.record1.featuretype\n",
    "axes = x3pobj7.record1.axes.CX.axistype\n",
    "data = x3pobj7.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df7 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df7 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef26085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame into a 1D array and remove NaN values\n",
    "flattened_data = df7.values.flatten()\n",
    "clean_data = flattened_data[~np.isnan(flattened_data)]  # Remove NaN values\n",
    "\n",
    "# Calculate the mean and standard deviation for the cleaned dataset\n",
    "overall_mean = clean_data.mean()\n",
    "overall_std = clean_data.std()\n",
    "\n",
    "# Display the results\n",
    "print(\"Overall Mean (excluding NaNs):\", overall_mean)\n",
    "print(\"Overall Standard Deviation (excluding NaNs):\", overall_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d87e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 8 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj8 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj8.data.shape) > 2:\n",
    "    axest = x3pobj8.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj8.data.shape[0]):\n",
    "            plt.imshow(x3pobj8.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj8.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj8.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj8.record1.featuretype\n",
    "axes = x3pobj8.record1.axes.CX.axistype\n",
    "data = x3pobj8.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df8 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df8 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80edc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame into a 1D array and remove NaN values\n",
    "flattened_data = df8.values.flatten()\n",
    "clean_data = flattened_data[~np.isnan(flattened_data)]  # Remove NaN values\n",
    "\n",
    "# Calculate the mean and standard deviation for the cleaned dataset\n",
    "overall_mean = clean_data.mean()\n",
    "overall_std = clean_data.std()\n",
    "\n",
    "# Display the results\n",
    "print(\"Overall Mean (excluding NaNs):\", overall_mean)\n",
    "print(\"Overall Standard Deviation (excluding NaNs):\", overall_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Comparison: If the images are not too large, a simple visual comparison might be insightful. You could display them side by side for a qualitative assessment. This method is subjective but can be useful for a quick comparison.\n",
    "\n",
    "# Statistical Comparison: You can compute statistical metrics for each image and compare these metrics. Common metrics include the mean, median, standard deviation, and range of the surface heights. Differences in these metrics can give you an idea of the differences in surface roughness.\n",
    "\n",
    "# Histogram Comparison: Generate histograms of the surface height values for each image. This will give you an idea of the distribution of heights in each image. Comparing these histograms can provide insights into the differences in surface roughness.\n",
    "\n",
    "# Correlation Analysis: If the images are of the same size and correspond to the same surface area, you could calculate the correlation coefficient between the two datasets. A high correlation might indicate similar surface structures, while a low correlation could suggest differences.\n",
    "\n",
    "# Image Difference: You can subtract one image from the other (assuming they are aligned and of the same size) to get a difference image. This can highlight the areas where the two surfaces differ most significantly.\n",
    "\n",
    "# Texture Analysis: More advanced analysis might involve texture analysis methods, such as calculating the Gray Level Co-occurrence Matrix (GLCM) or other texture descriptors. These can quantify aspects of surface roughness more precisely.\n",
    "\n",
    "# 3D Surface Plotting: If the data allows, creating 3D surface plots of each image can provide a more intuitive understanding of the surface topography. Differences in the 3D plots might be easier to perceive and analyze than in 2D.\n",
    "\n",
    "# Machine Learning Models: For a more sophisticated analysis, machine learning models can be trained to classify or quantify surface roughness based on the height data. This approach would require a dataset labeled with roughness metrics.\n",
    "\n",
    "# Fourier Analysis: You can use Fourier Transform to analyze the frequency components of the surface height data. This method can help in understanding the spatial frequency characteristics of the surfaces, which can be related to roughness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame into a 1D array and remove NaN values for df7\n",
    "flattened_data_7 = df7.values.flatten()\n",
    "clean_data_7 = flattened_data_7[~np.isnan(flattened_data_7)]\n",
    "\n",
    "# Calculate the mean and standard deviation for df7\n",
    "mean_7 = clean_data_7.mean()\n",
    "std_7 = clean_data_7.std()\n",
    "\n",
    "# Flatten the DataFrame into a 1D array and remove NaN values for df8\n",
    "flattened_data_8 = df8.values.flatten()\n",
    "clean_data_8 = flattened_data_8[~np.isnan(flattened_data_8)]\n",
    "\n",
    "# Calculate the mean and standard deviation for df8\n",
    "mean_8 = clean_data_8.mean()\n",
    "std_8 = clean_data_8.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Image 7 - Mean:\", mean_7, \"Standard Deviation:\", std_7)\n",
    "print(\"Image 8 - Mean:\", mean_8, \"Standard Deviation:\", std_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf283a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming clean_data_7 and clean_data_8 are already defined and cleaned\n",
    "\n",
    "# Plot histograms for each image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for Image 7\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.hist(clean_data_7, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of Surface Heights for Image 7')\n",
    "plt.xlabel('Height Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for Image 8\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.hist(clean_data_8, bins=50, color='green', alpha=0.7)\n",
    "plt.title('Histogram of Surface Heights for Image 8')\n",
    "plt.xlabel('Height Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "# As the dataset is large, we'll take a random sample of data points to perform the test\n",
    "# The Shapiro-Wilk test can be slow and may not be suitable for large datasets\n",
    "\n",
    "# Taking a sample of 5000 data points from each dataset for the test\n",
    "sw_stat_7, sw_p_value_7 = shapiro(np.random.choice(clean_data_7, 5000, replace=False))\n",
    "sw_stat_8, sw_p_value_8 = shapiro(np.random.choice(clean_data_8, 5000, replace=False))\n",
    "\n",
    "# Display the results\n",
    "print(f\"Shapiro-Wilk Test for Image 7: Statistic={sw_stat_7}, p-value={sw_p_value_7}\")\n",
    "print(f\"Shapiro-Wilk Test for Image 8: Statistic={sw_stat_8}, p-value={sw_p_value_8}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming clean_data_7 and clean_data_8 are already defined and cleaned\n",
    "\n",
    "# Define the range for the histograms based on the combined data of both images\n",
    "combined_data = np.concatenate([clean_data_7, clean_data_8])\n",
    "hist_range = (combined_data.min(), combined_data.max())\n",
    "\n",
    "# Plot histograms for each image\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for Image 7\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.hist(clean_data_7, bins=50, color='blue', alpha=0.7, range=hist_range)\n",
    "plt.title('Histogram of Surface Heights for Image 7')\n",
    "plt.xlabel('Height Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for Image 8\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.hist(clean_data_8, bins=50, color='green', alpha=0.7, range=hist_range)\n",
    "plt.title('Histogram of Surface Heights for Image 8')\n",
    "plt.xlabel('Height Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6bf03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d24249f",
   "metadata": {},
   "source": [
    "# comparing all the 9 surfaces of a rib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 8 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj8 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj8.data.shape) > 2:\n",
    "    axest = x3pobj8.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj8.data.shape[0]):\n",
    "            plt.imshow(x3pobj8.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj8.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj8.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj8.record1.featuretype\n",
    "axes = x3pobj8.record1.axes.CX.axistype\n",
    "data = x3pobj8.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df8 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df8 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df8.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 1 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj1 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj1.data.shape) > 2:\n",
    "    axest = x3pobj1.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj1.data.shape[0]):\n",
    "            plt.imshow(x3pobj1.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj1.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj1.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj1.record1.featuretype\n",
    "axes = x3pobj1.record1.axes.CX.axistype\n",
    "data = x3pobj1.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df1 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df1 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b17c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 2 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj2 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj2.data.shape) > 2:\n",
    "    axest = x3pobj2.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj2.data.shape[0]):\n",
    "            plt.imshow(x3pobj2.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj2.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj2.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj2.record1.featuretype\n",
    "axes = x3pobj2.record1.axes.CX.axistype\n",
    "data = x3pobj2.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df2 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df2 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 3 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj3 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj3.data.shape) > 2:\n",
    "    axest = x3pobj3.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj3.data.shape[0]):\n",
    "            plt.imshow(x3pobj3.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj3.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj3.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj3.record1.featuretype\n",
    "axes = x3pobj3.record1.axes.CX.axistype\n",
    "data = x3pobj3.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df3 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df3 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eaff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 4 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj4 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj4.data.shape) > 2:\n",
    "    axest = x3pobj4.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj4.data.shape[0]):\n",
    "            plt.imshow(x3pobj4.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj4.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj4.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj4.record1.featuretype\n",
    "axes = x3pobj4.record1.axes.CX.axistype\n",
    "data = x3pobj4.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df4 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df4 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 5 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj5 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj5.data.shape) > 2:\n",
    "    axest = x3pobj5.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj5.data.shape[0]):\n",
    "            plt.imshow(x3pobj5.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj5.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj5.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj5.record1.featuretype\n",
    "axes = x3pobj5.record1.axes.CX.axistype\n",
    "data = x3pobj5.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df5 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df5 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 6 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj6 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj6.data.shape) > 2:\n",
    "    axest = x3pobj6.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj6.data.shape[0]):\n",
    "            plt.imshow(x3pobj6.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj6.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj6.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame8\n",
    "feature_type = x3pobj6.record1.featuretype\n",
    "axes = x3pobj6.record1.axes.CX.axistype\n",
    "data = x3pobj6.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df6 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df6 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df6.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26a0d1-cd06-41ff-9d3a-be84b4b424cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 7 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj7 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj7.data.shape) > 2:\n",
    "    axest = x3pobj7.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj7.data.shape[0]):\n",
    "            plt.imshow(x3pobj7.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj7.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj7.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj7.record1.featuretype\n",
    "axes = x3pobj7.record1.axes.CX.axistype\n",
    "data = x3pobj7.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df7 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df7 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df7.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0bd01-7398-4fec-aed4-1f2c6527517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 8 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj8 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj8.data.shape) > 2:\n",
    "    axest = x3pobj8.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj8.data.shape[0]):\n",
    "            plt.imshow(x3pobj8.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj8.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj8.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj8.record1.featuretype\n",
    "axes = x3pobj8.record1.axes.CX.axistype\n",
    "data = x3pobj8.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df8 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df8 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df8.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d7b39-fad1-42bf-86e9-9ecf33eee40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aeaeb3-f458-4eee-821b-d0730ddf1951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f031d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # Import pandas for DataFrame creation\n",
    "from x3p import X3Pfile  # Import the library for handling .x3p files\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 9 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj9 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    # Exit or handle the error if the file can't be loaded\n",
    "    raise\n",
    "\n",
    "# Visualize the data\n",
    "if len(x3pobj9.data.shape) > 2:\n",
    "    axest = x3pobj9.record1.axes.get_XYaxes_types()\n",
    "    if axest == ['I', 'I']:\n",
    "        for i in range(x3pobj9.data.shape[0]):\n",
    "            plt.imshow(x3pobj9.data[i, :, :])\n",
    "            plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "            plt.title(f\"{filename} layer {i+1}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.imshow(x3pobj9.data[0, :, :])\n",
    "        plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "        plt.title(filename)\n",
    "        plt.show()\n",
    "else:\n",
    "    plt.imshow(x3pobj9.data)\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.title(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Extract information and convert data to DataFrame\n",
    "feature_type = x3pobj9.record1.featuretype\n",
    "axes = x3pobj9.record1.axes.CX.axistype\n",
    "data = x3pobj9.data\n",
    "\n",
    "# If the data is 2D or 3D, you might want to flatten it for a DataFrame\n",
    "if len(data.shape) > 1:\n",
    "    df9 = pd.DataFrame(data=data.reshape(-1, data.shape[-1]))  # Reshape for 2D or 3D data\n",
    "else:\n",
    "    df9 = pd.DataFrame(data=data)  # Directly use the data if it's 1D\n",
    "\n",
    "df9.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "# Assuming x3pobj1, x3pobj2, ..., x3pobj9 are already defined and loaded with their respective data\n",
    "\n",
    "# Set up a figure with multiple subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))  # Adjust the size as needed\n",
    "fig.suptitle('Comparison of Surface Images')\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# List of all x3p objects\n",
    "x3pobjs = [x3pobj1, x3pobj2, x3pobj3, x3pobj4, x3pobj5, x3pobj6, x3pobj7, x3pobj8, x3pobj9]\n",
    "\n",
    "# Generate captions using lowercase alphabet letters\n",
    "captions = [f'{letter})' for letter in string.ascii_lowercase[:len(x3pobjs)]]\n",
    "\n",
    "# Display each image in its respective subplot\n",
    "for i, (x3pobj, caption) in enumerate(zip(x3pobjs, captions)):\n",
    "    if len(x3pobj.data.shape) > 2:\n",
    "        axes[i].imshow(x3pobj.data[0, :, :])  # Assuming you want to display the first layer if it's 3D\n",
    "    else:\n",
    "        axes[i].imshow(x3pobj.data)\n",
    "    axes[i].invert_yaxis()  # Invert the y-axis\n",
    "    axes[i].set_title(caption)\n",
    "    axes[i].axis('off')  # Turn off axis for clarity\n",
    "\n",
    "plt.tight_layout()\n",
    "# Adjust the subplots area to give some more room for the suptitle:\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81284cc-76d2-4879-aba7-2b7a526efb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653984e-8a16-4136-b83b-f1be2df12fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import string\n",
    "\n",
    "# Set up a figure with multiple subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))  # Adjust the size as needed\n",
    "fig.suptitle('Comparison of Surface Images for the Part 1 - Rib 1')\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# List of all x3p objects\n",
    "x3pobjs = [x3pobj1, x3pobj2, x3pobj3, x3pobj4, x3pobj5, x3pobj6, x3pobj7, x3pobj8, x3pobj9]\n",
    "\n",
    "# Generate captions using lowercase alphabet letters\n",
    "captions = [f'{letter})' for letter in string.ascii_lowercase[:len(x3pobjs)]]\n",
    "\n",
    "# Display each image in its respective subplot and add caption with contrasting outline\n",
    "for i, (x3pobj, caption) in enumerate(zip(x3pobjs, captions)):\n",
    "    if len(x3pobj.data.shape) > 2:\n",
    "        axes[i].imshow(x3pobj.data[0, :, :])  # Assuming you want to display the first layer if it's 3D\n",
    "    else:\n",
    "        axes[i].imshow(x3pobj.data)\n",
    "    axes[i].invert_yaxis()  # Invert the y-axis\n",
    "    \n",
    "    # Add the caption with a path effect to create a contrasting outline\n",
    "    txt = axes[i].text(0.05, 0.95, caption, transform=axes[i].transAxes, fontsize=30, fontweight='bold', va='top', ha='left', color='red')\n",
    "    txt.set_path_effects([PathEffects.withStroke(linewidth=3, foreground='white')])\n",
    "    \n",
    "    axes[i].axis('off')  # Turn off axis for clarity\n",
    "\n",
    "plt.tight_layout()\n",
    "# Adjust the subplots area to give some more room for the suptitle:\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558baca-4373-40d0-8adf-bb2a511d57f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1fd866-c245-444f-b6ed-485c861ce1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e233bb-448a-4e3a-96f6-0c49c86c54f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707be1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df1, df2, ..., df9 are your DataFrames\n",
    "dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "stats_summary_list = []\n",
    "\n",
    "for i, df in enumerate(dataframes, 1):\n",
    "    flattened_data = df.values.flatten()\n",
    "    clean_data = flattened_data[~pd.isnull(flattened_data)]  # Remove NaN values if any\n",
    "    stats_summary_list.append({\n",
    "        \"DataFrame\": f\"Image {i}\",\n",
    "        \"Mean\": clean_data.mean(),\n",
    "        \"Standard Deviation\": clean_data.std()\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "stats_summary = pd.DataFrame(stats_summary_list)\n",
    "\n",
    "print(stats_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef01adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df1, df2, ..., df9 are your DataFrames\n",
    "dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "nan_counts = []\n",
    "\n",
    "for i, df in enumerate(dataframes, 1):\n",
    "    # Count NaN values in each DataFrame\n",
    "    total_values = df.size\n",
    "    nan_count = df.isna().sum().sum()  # Total count of NaNs in the DataFrame\n",
    "    nan_percentage = (nan_count / total_values) * 100  # Percentage of NaN values\n",
    "    nan_counts.append({\n",
    "        \"Image\": f\"Image {i}\",\n",
    "        \"NaN Count\": nan_count,\n",
    "        \"NaN Percentage\": nan_percentage\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier sorting and display\n",
    "nan_counts_df = pd.DataFrame(nan_counts)\n",
    "\n",
    "# Sort by NaN count in descending order\n",
    "sorted_nan_counts_df = nan_counts_df.sort_values(by=\"NaN Count\", ascending=False)\n",
    "\n",
    "print(sorted_nan_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64fe40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df1, df2, ..., df9 are your DataFrames\n",
    "dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "\n",
    "# Clean data and calculate the combined range\n",
    "clean_data_combined = np.concatenate([df.dropna().values.flatten() for df in dataframes])\n",
    "if len(clean_data_combined) > 0:\n",
    "    hist_range = (clean_data_combined.min(), clean_data_combined.max())\n",
    "else:\n",
    "    raise ValueError(\"No valid data found in DataFrames.\")\n",
    "\n",
    "# Set up a figure with multiple subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "fig.suptitle('Histograms of Surface Heights for Each Image')\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display each histogram in its respective subplot\n",
    "for i, df in enumerate(dataframes):\n",
    "    flattened_data = df.values.flatten()\n",
    "    clean_data = flattened_data[~np.isnan(flattened_data)]  # Remove NaN values\n",
    "    if len(clean_data) > 0:\n",
    "        axes[i].hist(clean_data, bins=50, color='blue', alpha=0.7, range=hist_range)\n",
    "        axes[i].set_title(f'Image {i+1}')\n",
    "        axes[i].set_xlabel('Height Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, 'No data available', horizontalalignment='center', verticalalignment='center')\n",
    "        axes[i].set_title(f'Image {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e667c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce108ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca218e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaec0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89a2388",
   "metadata": {},
   "source": [
    "# how are the images captured:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72c1bf",
   "metadata": {},
   "source": [
    "Measurement Equipment: An Alicona InfiniteFocus G5 focus variation microscope is used. This equipment is capable of capturing high-resolution, 3D surface topography data.\n",
    "\n",
    "Sample Mounting and Measurement: For ease of measurement, artifacts are attached to a bolt and then held in the microscope’s rotation unit. This allows for tilt and rotation of the part to enable measurements with the microscope's objective axis normal to the surface.\n",
    "\n",
    "Light Settings and Resolution: Light settings are adjusted for each surface to minimize data dropout. The lateral resolution setting of the microscope is set to 1.5 μm and point spacing to 0.5 μm, with vertical resolution at 0.1 μm.\n",
    "\n",
    "Surface Height Maps Creation: Surface height maps are created by the microscope software by stitching together an 8 x 8 field-of-view set of measurements.\n",
    "\n",
    "Data Cropping and Processing: Data is exported from the microscope's software in X3P format and loaded into MATLAB for processing. The center position of the dataset is determined and data is cropped to remove edges, resulting in a focused dataset for analysis.\n",
    "\n",
    "X3P File Format: This format is used for storing the surface data. The X3P format is a standard in surface metrology and allows for consistent and accurate representation of surface texture data.\n",
    "\n",
    "Dataset Orientation: Surface data files are set such that the build direction of the part aligns with the positive y-direction of the measurement data. The files are transposed/flipped as necessary to maintain this orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485f53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17a1f096",
   "metadata": {},
   "source": [
    "# why do we see alot of nan values in the surface 8 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d168f9b",
   "metadata": {},
   "source": [
    "Surface Angle and Orientation: In additive manufacturing, especially in processes like laser powder bed fusion, the angle and orientation of the surface relative to the build plate can significantly affect the quality of the finish. Surfaces with steep angles or orientations that are not optimal for the laser's path might result in poorer quality finishes, which could lead to more data dropouts or unmeasurable areas when scanned.\n",
    "\n",
    "Measurement Challenges: The focus variation microscopy technique used for these measurements might have limitations when dealing with certain surface textures or angles. If surfaces 8 and 9 have features (like high roughness, steep slopes, or undercuts) that are challenging to capture with this method, it could result in higher occurrences of NaN values.\n",
    "\n",
    "Material Deposition Irregularities: Inconsistencies in the powder bed fusion process, such as uneven powder layering, powder spattering, or issues with the laser's interaction with the material, can lead to irregularities on the surface. These irregularities might not be captured accurately during scanning, resulting in NaN values.\n",
    "\n",
    "Post-Processing Effects: If any post-processing steps (like support removal or surface treatments) were applied differently or less effectively to surfaces 8 and 9, this could impact the quality of the surface and the effectiveness of the measurement.\n",
    "\n",
    "Instrument Sensitivity and Settings: The microscope settings (like lighting, focus, and resolution) might need fine-tuning for different surfaces. If the settings were optimized for other surfaces but not for surfaces 8 and 9, it could result in poorer data capture for these specific surfaces.\n",
    "\n",
    "Data Processing and Cropping: If the data cropping process in MATLAB was not accurately aligned or appropriately scaled for these surfaces, it might have led to a higher proportion of NaN values being included in the final dataset.\n",
    "\n",
    "Surface Complexity: Some surfaces might inherently have more complex features (like cavities or sharp edges) that are difficult to measure accurately, leading to more NaN values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6601d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02f56fb",
   "metadata": {},
   "source": [
    "# levelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01917fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def plane(xy, a, b, c):\n",
    "    x, y = xy\n",
    "    return a*x + b*y + c\n",
    "\n",
    "def level_dataframe(df):\n",
    "    # Generating x and y grid coordinates\n",
    "    x, y = np.meshgrid(np.arange(df.shape[1]), np.arange(df.shape[0]))\n",
    "    z = df.values.flatten()\n",
    "\n",
    "    # Filtering NaN and infinite values\n",
    "    mask = ~np.isnan(z) & ~np.isinf(z)\n",
    "    xy_filtered = np.vstack([x.ravel()[mask], y.ravel()[mask]])\n",
    "    z_filtered = z[mask]\n",
    "\n",
    "    # Fit the plane\n",
    "    if xy_filtered.shape[1] > 3:  # More data points than parameters in the model\n",
    "        popt, _ = curve_fit(plane, xy_filtered, z_filtered)\n",
    "        z_fitted = plane(np.vstack([x.ravel(), y.ravel()]), *popt).reshape(df.shape)\n",
    "    else:\n",
    "        raise ValueError(\"Not enough data points for plane fitting\")\n",
    "\n",
    "    # Subtract the fitted plane\n",
    "    z_levelled = df - z_fitted\n",
    "\n",
    "    return z_levelled\n",
    "\n",
    "# Apply levelling to one of your DataFrames\n",
    "df1_levelled = level_dataframe(df1)\n",
    "\n",
    "# Plotting the original and levelled data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(df1, cmap='viridis')\n",
    "ax1.set_title('Original Data')\n",
    "ax1.invert_yaxis()  # Invert y-axis\n",
    "\n",
    "ax2.imshow(df1_levelled, cmap='viridis')\n",
    "ax2.set_title('Levelled Data')\n",
    "ax2.invert_yaxis()  # Invert y-axis\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcdb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def plane(xy, a, b, c):\n",
    "    x, y = xy\n",
    "    return a*x + b*y + c\n",
    "\n",
    "def level_dataframe(df):\n",
    "    x, y = np.meshgrid(np.arange(df.shape[1]), np.arange(df.shape[0]))\n",
    "    z = df.values.flatten()\n",
    "\n",
    "    # Exclude NaN values\n",
    "    mask = ~np.isnan(z)\n",
    "    xy_filtered = np.vstack([x.ravel()[mask], y.ravel()[mask]])\n",
    "    z_filtered = z[mask]\n",
    "\n",
    "    if xy_filtered.shape[1] > 3:\n",
    "        popt, _ = curve_fit(plane, xy_filtered, z_filtered)\n",
    "        z_fitted = plane(xy_filtered, *popt)\n",
    "        # Reshape mask to 2D\n",
    "        mask_2d = mask.reshape(df.shape)\n",
    "        z_fitted_full = np.full(df.shape, np.nan)  # Initialize an array full of NaNs\n",
    "        z_fitted_full[mask_2d] = z_fitted  # Replace non-NaN positions with fitted values\n",
    "        return df - z_fitted_full\n",
    "    else:\n",
    "        raise ValueError(\"Not enough data points for plane fitting\")\n",
    "\n",
    "# Replace this with loading your actual DataFrame\n",
    "df9 = pd.DataFrame(np.random.rand(8001, 8001))  # Example placeholder\n",
    "\n",
    "# Apply leveling to df9\n",
    "df9_levelled = level_dataframe(df9)\n",
    "\n",
    "# Adjusted color range\n",
    "vmin, vmax = np.nanmin(df9_levelled.values), np.nanmax(df9_levelled.values)\n",
    "\n",
    "# Print out some statistics\n",
    "print(\"Original Data Stats:\")\n",
    "print(\"Min:\", np.nanmin(df9.values))\n",
    "print(\"Max:\", np.nanmax(df9.values))\n",
    "print(\"Mean:\", np.nanmean(df9.values))\n",
    "print(\"Standard Deviation:\", np.nanstd(df9.values))\n",
    "\n",
    "print(\"\\nLevelled Data Stats:\")\n",
    "print(\"Min:\", np.nanmin(df9_levelled.values))\n",
    "print(\"Max:\", np.nanmax(df9_levelled.values))\n",
    "print(\"Mean:\", np.nanmean(df9_levelled.values))\n",
    "print(\"Standard Deviation:\", np.nanstd(df9_levelled.values))\n",
    "\n",
    "# Visualization with adjusted color range\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(df9, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "ax1.set_title('Original Data')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.imshow(df9_levelled, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "ax2.set_title('Levelled Data')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Histograms for data distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df9.values.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of Original Data')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df9_levelled.values.flatten(), bins=50, color='green', alpha=0.7)\n",
    "plt.title('Histogram of Levelled Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c2311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db18f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# Function to apply leveling to the image\n",
    "def level_image(data):\n",
    "    # Assuming 'data' is a 2D numpy array representing the image\n",
    "    # Create a meshgrid of coordinates\n",
    "    rows, cols = data.shape\n",
    "    c, r = np.meshgrid(np.arange(cols), np.arange(rows))\n",
    "\n",
    "    # Flatten the data and coordinates\n",
    "    flat_data = data.ravel()\n",
    "    flat_r = r.ravel()\n",
    "    flat_c = c.ravel()\n",
    "\n",
    "    # Fit a plane to the data\n",
    "    A = np.column_stack((flat_r, flat_c, np.ones_like(flat_data)))\n",
    "    plane_coeff, _, _, _ = np.linalg.lstsq(A, flat_data, rcond=None)\n",
    "\n",
    "    # Calculate the fitted plane values\n",
    "    plane = (plane_coeff[0] * r + plane_coeff[1] * c + plane_coeff[2])\n",
    "\n",
    "    # Subtract the plane from the original data to level it\n",
    "    leveled_data = data - plane\n",
    "    return leveled_data\n",
    "\n",
    "# Function to apply Gaussian filter\n",
    "def apply_gaussian_filter(data, sigma=1):\n",
    "    return gaussian_filter(data, sigma=sigma)\n",
    "\n",
    "# Name of the specific .x3p file to be processed\n",
    "filename = 'STV1 Rib 1 Surf 9 subset-t.x3p'\n",
    "\n",
    "# Try to load the .x3p file\n",
    "try:\n",
    "    x3pobj9 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Visualize the original data\n",
    "data = x3pobj9.data\n",
    "plt.imshow(data)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"Original - {filename}\")\n",
    "plt.show()\n",
    "\n",
    "# Apply leveling\n",
    "leveled_data = level_image(data)\n",
    "\n",
    "# Apply Gaussian filter\n",
    "filtered_data = apply_gaussian_filter(leveled_data, sigma=1)\n",
    "\n",
    "# Visualize the processed data\n",
    "plt.imshow(filtered_data)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"Leveled and Gaussian Filtered - {filename}\")\n",
    "plt.show()\n",
    "\n",
    "# Convert data to DataFrame\n",
    "# Assuming 'filtered_data' is the final processed data\n",
    "if len(filtered_data.shape) > 1:\n",
    "    df9 = pd.DataFrame(data=filtered_data.reshape(-1, filtered_data.shape[-1]))\n",
    "else:\n",
    "    df9 = pd.DataFrame(data=filtered_data)\n",
    "\n",
    "df9.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f34e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = df9.isna().mean().mean() * 100\n",
    "print(f\"Percentage of NaNs in the DataFrame: {nan_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c50584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize data\n",
    "def normalize_data(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "# Apply leveling\n",
    "leveled_data = level_image(data)\n",
    "\n",
    "# Normalize leveled data  \n",
    "normalized_leveled_data = normalize_data(leveled_data)\n",
    "\n",
    "# Apply Gaussian filter\n",
    "filtered_data = apply_gaussian_filter(normalized_leveled_data, sigma=1)\n",
    "\n",
    "# Normalize filtered data\n",
    "normalized_filtered_data = normalize_data(filtered_data)\n",
    "\n",
    "# Visualize the processed data\n",
    "plt.imshow(normalized_filtered_data)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"Leveled, Gaussian Filtered and Normalized - {filename}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dac4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# Function to apply simple leveling to the image\n",
    "def level_image_simple(data):\n",
    "    return data - np.mean(data)\n",
    "\n",
    "# Function to apply Gaussian filter\n",
    "def apply_gaussian_filter(data, sigma=1):\n",
    "    return gaussian_filter(data, sigma=sigma)\n",
    "\n",
    "# Function to normalize data\n",
    "def normalize_data(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# Load the .x3p file\n",
    "filename = 'STV1 Rib 1 Surf 9 subset-t.x3p'\n",
    "try:\n",
    "    x3pobj9 = X3Pfile(filename)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading {filename}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Original data\n",
    "data = x3pobj9.data\n",
    "plt.imshow(data)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"Original - {filename}\")\n",
    "plt.show()\n",
    "\n",
    "# Apply simple leveling\n",
    "leveled_data = level_image_simple(data)\n",
    "\n",
    "# Normalize leveled data\n",
    "normalized_leveled_data = normalize_data(leveled_data)\n",
    "\n",
    "# Experiment with different sigma values for Gaussian filter\n",
    "for sigma in [0.5, 1, 2, 5]:\n",
    "    filtered_data = apply_gaussian_filter(normalized_leveled_data, sigma=sigma)\n",
    "    normalized_filtered_data = normalize_data(filtered_data)\n",
    "\n",
    "    # Visualize\n",
    "    plt.imshow(normalized_filtered_data)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Sigma: {sigma}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb4893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b516f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca3f58d-258c-4581-9666-ff290f6a020e",
   "metadata": {},
   "source": [
    "Outlier Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40164ac0-3820-4638-97f0-0c27977d224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# List of filenames\n",
    "filenames = ['STV1 Rib 3 Surf 8 subset-t.x3p', 'STV2 Rib 3 Surf 8 subset-t.x3p', 'STV3 Rib 1 Surf 8 subset-t.x3p','STV4 Rib 3 Surf 8 subset-t.x3p','STV5 Rib 5 Surf 8 subset-t.x3p','STV6 Rib 5 Surf 8 subset-t.x3p','STV7 Rib 8 Surf 2 subset-t.x3p','STV8 Rib 4 Surf 8 subset-t.x3p', 'STV9 Rib 1 Surf 8 subset-t.x3p']  # Replace with actual filenames\n",
    "\n",
    "# Prepare the subplot layout\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))  # Adjust nrows and ncols based on the number of files\n",
    "axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "\n",
    "# Initialize a dictionary or list to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Process each file\n",
    "for i, filename in enumerate(filenames):\n",
    "    try:\n",
    "        x3pobj = X3Pfile(filename)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        continue  # Skip to the next file\n",
    "\n",
    "    # Visualization\n",
    "    ax = axes[i]  # Get the corresponding subplot axis\n",
    "    ax.imshow(x3pobj.data[0, :, :]) if len(x3pobj.data.shape) > 2 else ax.imshow(x3pobj.data)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(filename)\n",
    "\n",
    "    # DataFrame creation\n",
    "    data = x3pobj.data.reshape(-1, x3pobj.data.shape[-1]) if len(x3pobj.data.shape) > 1 else x3pobj.data\n",
    "    df = pd.DataFrame(data=data)\n",
    "    dataframes[filename] = df  # Store DataFrame in the dictionary\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Access individual DataFrames, e.g., dataframes['file1.x3p']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fcad6-7081-45e7-9998-4e13ecd051b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of NaN values in each DataFrame\n",
    "nan_percentages = {}\n",
    "\n",
    "for filename, df in dataframes.items():\n",
    "    total_values = df.size\n",
    "    nan_values = df.isna().sum().sum()  # Total number of NaN values in the DataFrame\n",
    "    nan_percentage = (nan_values / total_values) * 100  # Percentage of NaN values\n",
    "    nan_percentages[filename] = nan_percentage\n",
    "\n",
    "# Print the percentage of NaN values for each file\n",
    "for filename, nan_percentage in nan_percentages.items():\n",
    "    print(f\"{filename}: {nan_percentage:.2f}% NaN values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012191d-02fb-410a-8a58-8be437a6a819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e4c9e-5434-4394-ac10-2c65624ef84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a87a8-ca35-431b-ad39-0cff0f5be249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from x3p import X3Pfile\n",
    "\n",
    "def plane_leveling(xy, z_xy, frac_sample=.05):\n",
    "    # Random sampling of non-nan indices\n",
    "    idx = index_rand(z_xy, frac_sample)\n",
    "    \n",
    "    # Performing plane fit\n",
    "    prediction, parameters = plane_rand(z_xy, xy, idx)\n",
    "    \n",
    "    return prediction, parameters\n",
    "\n",
    "def index_rand(z_xy, frac_sample=.05):\n",
    "    # Define mask to exclude NaNs and obtain non-NaN indices\n",
    "    nan_mask = ~np.isnan(z_xy)\n",
    "    nan_indices = np.array(np.where(nan_mask)).flatten()\n",
    "\n",
    "    # Random sample setup\n",
    "    length = len(nan_indices)\n",
    "    size = int(length * frac_sample)\n",
    "    rng = np.random.default_rng()\n",
    "    idx = rng.choice(nan_indices, size=size, replace=False)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def plane_rand(z_xy, xy, idx):\n",
    "    # Calculating fit\n",
    "    model = sm.OLS(z_xy[idx], xy[idx], missing='drop').fit()\n",
    "\n",
    "    # Fit parameters\n",
    "    parameters = model.params\n",
    "    \n",
    "    # Predicted plane\n",
    "    prediction = model.predict(xy)\n",
    "    \n",
    "    return prediction, parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967741c-cb27-4c47-9706-2da70dc6a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .x3p file\n",
    "filename = 'STV1 Rib 1 Surf 9 subset-t.x3p'\n",
    "x3pobj9 = X3Pfile(filename)\n",
    "\n",
    "# Extract data and convert to numpy array\n",
    "data = np.array(x3pobj9.data, dtype='float64')\n",
    "\n",
    "# Create a meshgrid for x and y coordinates\n",
    "length = data.shape[0]  # Assuming square data\n",
    "x = np.arange(0, length)\n",
    "y = np.arange(0, length)\n",
    "xy = sm.add_constant(np.array(np.meshgrid(x, y)).T.reshape(-1, 2))  # Add a constant for fitting\n",
    "\n",
    "# Flatten the data to a 1D array for fitting\n",
    "z_xy = data.flatten()\n",
    "\n",
    "# Perform plane fitting and leveling\n",
    "frac_sample = 0.05  # Fraction of data to sample for fitting\n",
    "prediction, parameters = plane_leveling(xy, z_xy, frac_sample)\n",
    "z_xy_leveled = z_xy - prediction\n",
    "\n",
    "# Convert leveled data back to 2D (if needed)\n",
    "data_leveled = z_xy_leveled.reshape(length, length)\n",
    "\n",
    "# Create DataFrame from leveled data\n",
    "df_leveled = pd.DataFrame(data=data_leveled)\n",
    "\n",
    "# Display first few rows of the DataFrame\n",
    "df_leveled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238fbca-a29c-4838-835d-8a2ec2aa6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is the original DataFrame and df_leveled is the leveled DataFrame\n",
    "\n",
    "# Set up the matplotlib figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the original data\n",
    "ax1.imshow(data, cmap='viridis')\n",
    "ax1.set_title('Original Data')\n",
    "ax1.invert_yaxis()  # Inverting y-axis for consistency with typical image coordinates\n",
    "\n",
    "# Plot the leveled data\n",
    "ax2.imshow(df_leveled, cmap='viridis')\n",
    "ax2.set_title('Leveled Data')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af44b4-cf52-4cfd-89ec-e14da3b17f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacb3c3-f841-49ec-a3b9-8ad8132dbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .x3p file\n",
    "filename = 'STV7 Rib 8 Surf 2 subset-t.x3p'\n",
    "x3pobj9 = X3Pfile(filename)\n",
    "\n",
    "# Extract data and convert to numpy array\n",
    "data = np.array(x3pobj9.data, dtype='float64')\n",
    "\n",
    "# Create a meshgrid for x and y coordinates\n",
    "length = data.shape[0]  # Assuming square data\n",
    "x = np.arange(0, length)\n",
    "y = np.arange(0, length)\n",
    "xy = sm.add_constant(np.array(np.meshgrid(x, y)).T.reshape(-1, 2))  # Add a constant for fitting\n",
    "\n",
    "# Flatten the data to a 1D array for fitting\n",
    "z_xy = data.flatten()\n",
    "\n",
    "# Perform plane fitting and leveling\n",
    "frac_sample = 0.05  # Fraction of data to sample for fitting\n",
    "prediction, parameters = plane_leveling(xy, z_xy, frac_sample)\n",
    "z_xy_leveled = z_xy - prediction\n",
    "\n",
    "# Convert leveled data back to 2D (if needed)\n",
    "data_leveled = z_xy_leveled.reshape(length, length)\n",
    "\n",
    "# Create DataFrame from leveled data\n",
    "df_leveled = pd.DataFrame(data=data_leveled)\n",
    " \n",
    "# Display first few rows of the DataFrame\n",
    "# df_leveled.head()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is the original DataFrame and df_leveled is the leveled DataFrame\n",
    "\n",
    "# Set up the matplotlib figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the original data\n",
    "ax1.imshow(data, cmap='viridis')\n",
    "ax1.set_title('Original Data')\n",
    "ax1.invert_yaxis()  # Inverting y-axis for consistency with typical image coordinates\n",
    "\n",
    "# Plot the leveled data\n",
    "ax2.imshow(df_leveled, cmap='viridis')\n",
    "ax2.set_title('Leveled Data')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d80ba8-f552-4de7-a33b-8e7db4361a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5aeccc-03c1-4bd1-b84c-1b52c6d92347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9684f-f4e2-4b64-acc9-bcbee80dc200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb841ab0-1a2f-496e-a9a4-5e2ae398d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# List of filenames\n",
    "filenames = ['STV7 Rib 1 Surf 2 subset-t.x3p', 'STV7 Rib 2 Surf 2 subset-t.x3p', 'STV7 Rib 3 Surf 2 subset-t.x3p','STV7 Rib 4 Surf 2 subset-t.x3p','STV7 Rib 5 Surf 2 subset-t.x3p','STV7 Rib 6 Surf 2 subset-t.x3p','STV7 Rib 7 Surf 2 subset-t.x3p','STV7 Rib 8 Surf 2 subset-t.x3p']  \n",
    "\n",
    "# Prepare the subplot layout\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))  # Adjust nrows and ncols based on the number of files\n",
    "axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "\n",
    "# Initialize a dictionary or list to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Process each file\n",
    "for i, filename in enumerate(filenames):\n",
    "    try:\n",
    "        x3pobj = X3Pfile(filename)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        continue  # Skip to the next file\n",
    "\n",
    "    # Visualization\n",
    "    ax = axes[i]  # Get the corresponding subplot axis\n",
    "    ax.imshow(x3pobj.data[0, :, :]) if len(x3pobj.data.shape) > 2 else ax.imshow(x3pobj.data)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(filename)\n",
    "\n",
    "    # DataFrame creation\n",
    "    data = x3pobj.data.reshape(-1, x3pobj.data.shape[-1]) if len(x3pobj.data.shape) > 1 else x3pobj.data\n",
    "    df = pd.DataFrame(data=data)\n",
    "    dataframes[filename] = df  # Store DataFrame in the dictionary\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Access individual DataFrames, e.g., dataframes['file1.x3p']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ba5a2-24ed-459e-acdc-a92833a8287c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69984c46-50c4-4668-a55d-c5806ff93686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d561c4-99f2-406e-9184-d02838154c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4d559-d4e4-42ae-b19a-165a31a89861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac1f2b-f715-4fd7-9db2-a9c9f2c204ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4e4dc-0e4f-4a6a-90ce-7cb4b656ca73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f8101-9320-46d2-b10d-33e0aa54efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without replacing the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed0874-b2dc-43c6-ab52-d922285bc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_leveled is your 8001 x 8001 DataFrame\n",
    "# df_leveled = pd.DataFrame(data=data_leveled)  \n",
    "\n",
    "# Function to fragment the DataFrame and compute standard deviation\n",
    "def compute_std_blocks(df, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    std_values = np.zeros((nrows // block_size + 1, ncols // block_size + 1))\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "            std_values[i//block_size, j//block_size] = block.std().std()\n",
    "\n",
    "    return std_values\n",
    "\n",
    "# Compute standard deviations\n",
    "std_blocks = compute_std_blocks(df_leveled)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(std_blocks, cmap='viridis')\n",
    "plt.colorbar(label='Standard Deviation')\n",
    "plt.title('Standard Deviation of Each 100x100 Block')\n",
    "plt.show()\n",
    "\n",
    "# Function to retrieve blocks based on standard deviation\n",
    "def retrieve_blocks(df, std_blocks, std_value, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    retrieved_blocks = []\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            if std_blocks[i//block_size, j//block_size] == std_value:\n",
    "                block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "                retrieved_blocks.append(block)\n",
    "\n",
    "    return retrieved_blocks\n",
    "\n",
    "# Example: Retrieve blocks with a specific standard deviation value\n",
    "std_value_to_retrieve = 1.5  # Set this to your desired value\n",
    "blocks_retrieved = retrieve_blocks(df_leveled, std_blocks, std_value_to_retrieve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d6611-8ada-4b5b-b524-492f26ae1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05333360-b33f-4d6a-88b6-494da9aa8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_leveled is your 8001 x 8001 DataFrame\n",
    "# df_leveled = pd.DataFrame(data=data_leveled)  # Replace this with your actual DataFrame\n",
    "\n",
    "# Function to fragment the DataFrame and compute standard deviation\n",
    "def compute_std_blocks(df, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    std_values = np.zeros((nrows // block_size + 1, ncols // block_size + 1))\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "            block_filled = block.fillna(block.mean())  # Replace NaNs with block mean\n",
    "            std_values[i//block_size, j//block_size] = block_filled.std().std()\n",
    "\n",
    "    return std_values\n",
    "\n",
    "# Compute and plot as before\n",
    "std_blocks = compute_std_blocks(df_leveled)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(std_blocks, cmap='viridis')\n",
    "plt.colorbar(label='Standard Deviation')\n",
    "plt.title('Standard Deviation of Each 100x100 Block (NaNs Handled)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Function to retrieve blocks based on standard deviation\n",
    "def retrieve_blocks(df, std_blocks, std_value, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    retrieved_blocks = []\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            if std_blocks[i//block_size, j//block_size] == std_value:\n",
    "                block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "                retrieved_blocks.append(block)\n",
    "\n",
    "    return retrieved_blocks\n",
    "\n",
    "# Example: Retrieve blocks with a specific standard deviation value\n",
    "std_value_to_retrieve = 1.5  # Set this to your desired value\n",
    "blocks_retrieved = retrieve_blocks(df_leveled, std_blocks, std_value_to_retrieve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b7e8b-2874-4f50-9ae9-55147d3d122e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06594c14-13d8-4a20-b644-ad979da0cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_leveled is your 8001 x 8001 DataFrame\n",
    "def compute_std_values(df, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    std_values = []\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "            block_std = block.std().std()\n",
    "            std_values.append(block_std)\n",
    "\n",
    "    return std_values\n",
    "\n",
    "# Compute standard deviations\n",
    "std_values = compute_std_values(df_leveled)\n",
    "\n",
    "# Plotting as a line plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(std_values)\n",
    "plt.xlabel('Block Number')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title('Standard Deviation of Each 100x100 Block')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Function to retrieve blocks based on standard deviation\n",
    "def retrieve_blocks(df, std_blocks, std_value, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    retrieved_blocks = []\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            if std_blocks[i//block_size, j//block_size] == std_value:\n",
    "                block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "                retrieved_blocks.append(block)\n",
    "\n",
    "    return retrieved_blocks\n",
    "\n",
    "# Example: Retrieve blocks with a specific standard deviation value\n",
    "std_value_to_retrieve = 1.5  # Set this to your desired value\n",
    "blocks_retrieved = retrieve_blocks(df_leveled, std_blocks, std_value_to_retrieve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c8ccb-0bd5-4dfa-91a0-5c6fe795843c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c251a7f-9030-4283-8f92-9c72a35a271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_std_values(df, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    std_values = []\n",
    "    block_positions = []\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "            block_std = block.std(skipna=True).std(skipna=True)  # Skip NaN values\n",
    "            std_values.append(block_std)\n",
    "            block_positions.append((i, j))\n",
    "\n",
    "    return std_values, block_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cab956-a6e3-4d3b-b68d-63faf2423758",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_values, block_positions = compute_std_values(df_leveled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d23682-35e2-4867-9aa8-7951f34013cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out NaN values from std_values and keep track of original indices\n",
    "filtered_std_values = [(value, index) for index, value in enumerate(std_values) if not np.isnan(value)]\n",
    "\n",
    "# Separate the values and indices for further processing\n",
    "filtered_values, filtered_indices = zip(*filtered_std_values)\n",
    "\n",
    "# Compute statistics on the filtered list\n",
    "mean_std = np.mean(filtered_values)\n",
    "min_std = np.min(filtered_values)\n",
    "max_std = np.max(filtered_values)\n",
    "\n",
    "# Find the original positions of the min and max std values\n",
    "min_std_original_index = filtered_indices[filtered_values.index(min_std)]\n",
    "max_std_original_index = filtered_indices[filtered_values.index(max_std)]\n",
    "min_std_block_position = block_positions[min_std_original_index]\n",
    "max_std_block_position = block_positions[max_std_original_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c5702-2e05-43a3-b6e5-532690d545ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_block(df, top_left_position, block_size=100):\n",
    "    i, j = top_left_position\n",
    "    return df.iloc[i:i+block_size, j:j+block_size]\n",
    "\n",
    "# Example: Retrieve and visualize the block with the maximum standard deviation\n",
    "max_std_block = retrieve_block(df_leveled, max_std_block_position)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(max_std_block, cmap='viridis', origin='lower')  \n",
    "plt.colorbar()\n",
    "plt.title(\"Block with Maximum Standard Deviation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52656b6-b1c1-467e-88ec-b21f94d59449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_block(df, top_left_position, block_size=100):\n",
    "    i, j = top_left_position\n",
    "    return df.iloc[i:i+block_size, j:j+block_size]\n",
    "\n",
    "# Example: Retrieve and visualize the block with the maximum standard deviation\n",
    "min_std_block = retrieve_block(df_leveled, min_std_block_position)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(min_std_block, cmap='viridis', origin='lower')  \n",
    "plt.colorbar()\n",
    "plt.title(\"Block with Minimum Standard Deviation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3037c-6608-4557-a055-4e6d476851bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0380954-1044-4049-bc87-bc1fc6cb744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# Load the .x3p file\n",
    "filename = 'STV7 Rib 8 Surf 2 subset-t.x3p'\n",
    "x3pobj9 = X3Pfile(filename)\n",
    "\n",
    "# Extract data and convert to numpy array\n",
    "data = np.array(x3pobj9.data, dtype='float64')\n",
    "\n",
    "# Create DataFrame from original data\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "def compute_std_values(df, block_size=100, nan_threshold=0.5):\n",
    "    nrows, ncols = df.shape\n",
    "    std_values = []\n",
    "    block_positions = []\n",
    "\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "            if block.isna().mean().mean() < nan_threshold:  # Check if the block has too many NaNs\n",
    "                block_std = block.std().std()\n",
    "                std_values.append(block_std)\n",
    "                block_positions.append((i, j))\n",
    "\n",
    "    return std_values, block_positions\n",
    "\n",
    "# Compute standard deviations for the original DataFrame\n",
    "std_values, block_positions = compute_std_values(df)\n",
    "\n",
    "# Calculate the mean and standard deviation of the standard deviation values\n",
    "mean_std = np.nanmean(std_values)  # Using np.nanmean to ignore NaN values\n",
    "std_std = np.nanstd(std_values)  # Using np.nanstd to ignore NaN values\n",
    "\n",
    "\n",
    "# Identify outliers\n",
    "outlier_threshold = 1.96\n",
    "outliers_high = [(pos, std) for pos, std in zip(block_positions, std_values) \n",
    "                 if std > mean_std + outlier_threshold * std_std]\n",
    "outliers_low = [(pos, std) for pos, std in zip(block_positions, std_values) \n",
    "                if std < mean_std - outlier_threshold * std_std]\n",
    "\n",
    "# Combine both high and low outliers\n",
    "outliers = outliers_high + outliers_low\n",
    "\n",
    "# Function to create a mask for outlier blocks\n",
    "def create_outlier_mask(df, block_size, outliers):\n",
    "    mask = np.zeros(df.shape, dtype=bool)\n",
    "    for pos, _ in outliers:\n",
    "        i, j = pos\n",
    "        mask[i:i+block_size, j:j+block_size] = True\n",
    "    return mask\n",
    "\n",
    "# Function to remove outlier blocks\n",
    "def remove_outlier_blocks(df, mask):\n",
    "    df_no_outliers = df.copy()\n",
    "    df_no_outliers[mask] = np.nan  # Replace outliers with NaN\n",
    "    return df_no_outliers\n",
    "\n",
    "# Create a mask for the outliers\n",
    "outlier_mask = create_outlier_mask(df, 100, outliers)\n",
    "\n",
    "# Remove outlier blocks from the DataFrame\n",
    "df_no_outliers = remove_outlier_blocks(df, outlier_mask)\n",
    "\n",
    "# Convert the modified DataFrame back to a NumPy array for visualization\n",
    "data_no_outliers = df_no_outliers.to_numpy()\n",
    "\n",
    "# Plotting both original and modified data side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Original data\n",
    "im1 = ax1.imshow(data, cmap='viridis', origin='lower')\n",
    "ax1.set_title('Original Data')\n",
    "fig.colorbar(im1, ax=ax1)  # Corrected colorbar addition\n",
    "\n",
    "# Data with outliers removed\n",
    "im2 = ax2.imshow(data_no_outliers, cmap='viridis', origin='lower')\n",
    "ax2.set_title('Data with Outliers Removed')\n",
    "fig.colorbar(im2, ax=ax2)  # Corrected colorbar addition\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about outliers\n",
    "print(\"Number of Outliers:\", len(outliers))\n",
    "if len(outliers) > 0:\n",
    "    print(\"Outliers (Position, Std Deviation):\")\n",
    "    for pos, std in outliers:\n",
    "        print(pos, std)\n",
    "else:\n",
    "    print(\"No outliers found.\")\n",
    "\n",
    "# Optional: Print the mean and standard deviation of the std_values for debugging\n",
    "print(\"Mean of Standard Deviations:\", mean_std)\n",
    "print(\"Standard Deviation of Standard Deviations:\", std_std)\n",
    "\n",
    "# Optional: Inspect some of the standard deviation values\n",
    "print(\"Some Standard Deviation Values:\", std_values[:10])  # Prints the first 10 values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bbbbb-8cfa-4dc4-8c5f-ce08f2bd0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-NaN values before removing outliers\n",
    "initial_non_nan = np.count_nonzero(~np.isnan(data))\n",
    "\n",
    "# Number of non-NaN values after removing outliers\n",
    "final_non_nan = np.count_nonzero(~np.isnan(data_no_outliers))\n",
    "\n",
    "# Calculate the percentage of data lost\n",
    "data_lost_percentage = 100 * (initial_non_nan - final_non_nan) / initial_non_nan\n",
    "print(f\"Percentage of Data Lost: {data_lost_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca9e2f-4f2c-42ee-aa7c-33f84876655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame to a 1D array and compute the standard deviation\n",
    "overall_std = df_no_outliers.stack().std()\n",
    "\n",
    "print(\"Standard Deviation of the entire image after outliers removed:\", overall_std*1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e3af8-8413-4d15-be6f-9947f0713b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame to a 1D array and compute the standard deviation\n",
    "overall_std = df.stack().std()\n",
    "\n",
    "print(\"Standard Deviation of the entire image before outliers removed:\", overall_std*1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45aff89-ff0a-48d6-91fc-98d433ac4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# Load the .x3p file\n",
    "filename = 'STV7 Rib 8 Surf 2 subset-t.x3p'\n",
    "x3pobj9 = X3Pfile(filename)\n",
    "\n",
    "# Extract data and convert to numpy array\n",
    "data = np.array(x3pobj9.data, dtype='float64')\n",
    "\n",
    "# Create DataFrame from original data\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "def compute_block_stats(df, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    block_stats = {}\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "            block_mean = block.stack().mean()  # Use stack to ignore NaNs\n",
    "            block_std = block.stack().std()\n",
    "            block_stats[(i, j)] = (block_mean, block_std)\n",
    "    return block_stats\n",
    "\n",
    "# Compute mean and standard deviation for each block\n",
    "block_stats = compute_block_stats(df)\n",
    "\n",
    "# Function to remove outlier pixels\n",
    "def remove_outlier_pixels(df, block_size, block_stats, outlier_threshold=1.96):\n",
    "    df_no_outliers = df.copy()\n",
    "    for (i, j), (mean, std) in block_stats.items():\n",
    "        block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "        lower_bound = mean - outlier_threshold * std\n",
    "        upper_bound = mean + outlier_threshold * std\n",
    "        outlier_mask = ((block < lower_bound) | (block > upper_bound))\n",
    "        df_no_outliers.iloc[i:i+block_size, j:j+block_size][outlier_mask] = np.nan\n",
    "    return df_no_outliers\n",
    "\n",
    "# Remove outlier pixels from the DataFrame\n",
    "df_no_outliers = remove_outlier_pixels(df, 100, block_stats)\n",
    "\n",
    "# Convert the modified DataFrame back to a NumPy array for visualization\n",
    "data_no_outliers = df_no_outliers.to_numpy()\n",
    "\n",
    "# Plotting both original and modified data side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Original data\n",
    "im1 = ax1.imshow(data, cmap='viridis', origin='lower')\n",
    "ax1.set_title('Original Data')\n",
    "fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Data with outlier pixels removed\n",
    "im2 = ax2.imshow(data_no_outliers, cmap='viridis', origin='lower')\n",
    "ax2.set_title('Data with Outlier Pixels Removed')\n",
    "fig.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9280a3-0080-4bd1-a638-aad9e959b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('dataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28e2ab-3538-4aa8-8a8c-0ca435b009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract standard deviations from the block_stats dictionary\n",
    "std_devs = [stats[1] for stats in block_stats.values()]\n",
    "\n",
    "# Plotting the histogram of standard deviations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(std_devs, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of Standard Deviations for Each Block')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d89c5-6528-4737-bd25-58091b82f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract outlier pixels\n",
    "def extract_outlier_pixels(df, block_size, block_stats, outlier_threshold=1.96):\n",
    "    # Create an empty DataFrame to store outlier pixels, with the same shape as the original DataFrame\n",
    "    df_outliers = pd.DataFrame(np.nan, index=df.index, columns=df.columns)\n",
    "    \n",
    "    for (i, j), (mean, std) in block_stats.items():\n",
    "        block = df.iloc[i:i+block_size, j:j+block_size]\n",
    "        lower_bound = mean - outlier_threshold * std\n",
    "        upper_bound = mean + outlier_threshold * std\n",
    "        outlier_mask = ((block < lower_bound) | (block > upper_bound))\n",
    "        # Place outlier values in the df_outliers DataFrame\n",
    "        df_outliers.iloc[i:i+block_size, j:j+block_size][outlier_mask] = block[outlier_mask]\n",
    "        \n",
    "    return df_outliers\n",
    "\n",
    "# Extract outlier pixels\n",
    "df_outliers = extract_outlier_pixels(df, 100, block_stats)\n",
    "\n",
    "# Convert the outlier DataFrame back to a NumPy array for visualization\n",
    "data_outliers = df_outliers.to_numpy()\n",
    "\n",
    "# Plotting the outlier pixels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(data_outliers, cmap='viridis', origin='lower')\n",
    "plt.title('Outlier Pixels')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a6b4a-5c41-4053-944a-44a3b58d44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the outlier pixels DataFrame with the outlier-removed DataFrame\n",
    "# Since the outlier pixels DataFrame contains NaNs for non-outlier pixels,\n",
    "# adding it to the outlier-removed DataFrame should only fill in the previously removed outliers\n",
    "df_reconstructed = df_no_outliers.add(df_outliers, fill_value=0)\n",
    "\n",
    "# Calculate the standard deviation of the reconstructed DataFrame\n",
    "overall_std_reconstructed = df_reconstructed.stack().std()\n",
    "\n",
    "print(\"Standard Deviation of the reconstructed image:\", overall_std_reconstructed*1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10777225-53e5-46ec-9313-6bb21a50c411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bca8c8-24c8-4a60-a6d5-257a76ca8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-NaN values before removing outliers\n",
    "initial_non_nan = np.count_nonzero(~np.isnan(data))\n",
    "\n",
    "# Number of non-NaN values after removing outliers\n",
    "final_non_nan = np.count_nonzero(~np.isnan(data_no_outliers))\n",
    "\n",
    "# Calculate the percentage of data lost\n",
    "data_lost_percentage = 100 * (initial_non_nan - final_non_nan) / initial_non_nan\n",
    "print(f\"Percentage of Data Lost: {data_lost_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d73754-bb27-46b1-9670-a6b5e08b1047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e66e7-ca28-4ba9-b53b-6b0c716cf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame to a 1D array and compute the standard deviation\n",
    "overall_std = df_no_outliers.stack().std()\n",
    "\n",
    "print(\"Standard Deviation of the entire image after outliers removed:\", overall_std*1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a47338-d6df-42c0-a663-fe5ff977ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the DataFrame to a 1D array and compute the standard deviation\n",
    "overall_std = df.stack().std()\n",
    "\n",
    "print(\"Standard Deviation of the entire image before outliers removed:\", overall_std*1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801374d-3835-4805-b2e7-988c7e42b99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29e16b-a821-482b-a0ae-a3018a14c11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b805710-fef0-4998-8cb6-f5ada1ec1764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from x3p import X3Pfile\n",
    "\n",
    "def process_file(filename, block_size=100, outlier_threshold=1.96):\n",
    "    # Load the .x3p file\n",
    "    x3p_obj = X3Pfile(filename)\n",
    "    \n",
    "    # Extract data and convert to numpy array\n",
    "    data = np.array(x3p_obj.data, dtype='float64')\n",
    "    \n",
    "    # Create DataFrame from original data\n",
    "    df = pd.DataFrame(data=data)\n",
    "    \n",
    "    # Compute block statistics\n",
    "    block_stats = compute_block_stats(df, block_size)\n",
    "    \n",
    "    # Remove outlier pixels from the DataFrame\n",
    "    df_no_outliers = remove_outlier_pixels(df, block_size, block_stats, outlier_threshold)\n",
    "    \n",
    "    return data, df_no_outliers.to_numpy(), block_stats\n",
    "\n",
    "def plot_data(original_data, processed_data, title_suffix=''):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # Original data\n",
    "    im1 = ax1.imshow(original_data, cmap='viridis', origin='lower')\n",
    "    ax1.set_title(f'Original Data {title_suffix}')\n",
    "    fig.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Data with outlier pixels removed\n",
    "    im2 = ax2.imshow(processed_data, cmap='viridis', origin='lower')\n",
    "    ax2.set_title(f'Data with Outlier Pixels Removed {title_suffix}')\n",
    "    fig.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(block_stats, title_suffix=''):\n",
    "    # Extract standard deviations from the block_stats dictionary\n",
    "    std_devs = [stats[1] for stats in block_stats.values()]\n",
    "    \n",
    "    # Plotting the histogram of standard deviations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(std_devs, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title(f'Histogram of Standard Deviations for Each Block {title_suffix}')\n",
    "    plt.xlabel('Standard Deviation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "filenames = [\n",
    "    'STV7 Rib 1 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 2 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 3 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 4 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 5 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 6 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 7 Surf 2 subset-t.x3p',\n",
    "    'STV7 Rib 8 Surf 2 subset-t.x3p'\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    original_data, processed_data, block_stats = process_file(filename)\n",
    "    plot_data(original_data, processed_data, title_suffix=filename)\n",
    "    plot_histogram(block_stats, title_suffix=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8574fb-76c9-4660-86bd-6954d84e2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from x3p import X3Pfile\n",
    "\n",
    "def process_file(filename, block_size=100, outlier_threshold=1.96):\n",
    "    # Load the .x3p file\n",
    "    x3p_obj = X3Pfile(filename)\n",
    "    \n",
    "    # Extract data and convert to numpy array\n",
    "    data = np.array(x3p_obj.data, dtype='float64')\n",
    "    \n",
    "    # Create DataFrame from original data\n",
    "    df = pd.DataFrame(data=data)\n",
    "    \n",
    "    # Compute block statistics\n",
    "    block_stats = compute_block_stats(df, block_size)\n",
    "    \n",
    "    # Remove outlier pixels from the DataFrame\n",
    "    df_no_outliers = remove_outlier_pixels(df, block_size, block_stats, outlier_threshold)\n",
    "    \n",
    "    return data, df_no_outliers.to_numpy(), block_stats\n",
    "\n",
    "def plot_data(original_data, processed_data, title_suffix=''):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # Original data\n",
    "    im1 = ax1.imshow(original_data, cmap='viridis', origin='lower')\n",
    "    ax1.set_title(f'Original Data {title_suffix}')\n",
    "    fig.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Data with outlier pixels removed\n",
    "    im2 = ax2.imshow(processed_data, cmap='viridis', origin='lower')\n",
    "    ax2.set_title(f'Data with Outlier Pixels Removed {title_suffix}')\n",
    "    fig.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(block_stats, title_suffix=''):\n",
    "    # Extract standard deviations from the block_stats dictionary\n",
    "    std_devs = [stats[1] for stats in block_stats.values()]\n",
    "    \n",
    "    # Plotting the histogram of standard deviations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(std_devs, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title(f'Histogram of Standard Deviations for Each Block {title_suffix}')\n",
    "    plt.xlabel('Standard Deviation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "filenames = [\n",
    "    'STV1 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV2 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV3 Rib 1 Surf 8 subset-t.x3p',\n",
    "    'STV4 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV5 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV6 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV7 Rib 8 Surf 2 subset-t.x3p',\n",
    "    'STV8 Rib 4 Surf 8 subset-t.x3p',\n",
    "    'STV9 Rib 1 Surf 8 subset-t.x3p'\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    original_data, processed_data, block_stats = process_file(filename)\n",
    "    plot_data(original_data, processed_data, title_suffix=filename)\n",
    "    plot_histogram(block_stats, title_suffix=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5a8f1-8723-403b-a667-732af0455425",
   "metadata": {},
   "source": [
    "### trying different block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d43b1-fd10-4fc3-9954-b61add4221ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# Assume compute_block_stats and remove_outlier_pixels functions are defined elsewhere in your script\n",
    "\n",
    "filenames = [\n",
    "    'STV1 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV2 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV3 Rib 1 Surf 8 subset-t.x3p',\n",
    "    'STV4 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV5 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV6 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV7 Rib 8 Surf 2 subset-t.x3p',\n",
    "    'STV8 Rib 4 Surf 8 subset-t.x3p',\n",
    "    'STV9 Rib 1 Surf 8 subset-t.x3p'\n",
    "]\n",
    "\n",
    "block_sizes = [25, 50, 75]  # Different block sizes to try\n",
    "\n",
    "for block_size in block_sizes:\n",
    "    print(f\"Processing with block size: {block_size}x{block_size}\")\n",
    "    for filename in filenames:\n",
    "        original_data, processed_data, block_stats = process_file(filename, block_size=block_size)\n",
    "        # Modify title suffix to include block size information\n",
    "        title_suffix = f\"{filename} - Block Size {block_size}x{block_size}\"\n",
    "        plot_data(original_data, processed_data, title_suffix=title_suffix)\n",
    "        plot_histogram(block_stats, title_suffix=title_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a07e2-7685-4efe-831d-4ee732196d14",
   "metadata": {},
   "source": [
    "Overall Variability vs. Local Variability: The standard deviation of the entire image captures the overall variability across the entire dataset, which includes all variations, anomalies, and patterns within the data. On the other hand, the standard deviation calculated for individual blocks represents local variability, which might be lower if the blocks capture more homogeneous or less variable parts of the image.\n",
    "\n",
    "Outliers Influence: If the original data contains outliers or extreme values, they can significantly impact the overall standard deviation, making it much higher. When you remove these outliers or when calculating standard deviations over smaller blocks, these extreme values might be excluded, leading to a lower standard deviation for the blocks.\n",
    "\n",
    "Block Size and Selection: The size and the selection of blocks can also affect the observed standard deviation. Smaller blocks may capture less variability, especially if the image has areas of uniform texture or color. The way blocks are chosen (e.g., if they tend to cover more homogeneous regions) can also bias the standard deviations towards lower values.\n",
    "\n",
    "Averaging Effect: When plotting the histogram of standard deviations of blocks, you are essentially looking at a distribution of local variability measures. The mean of this distribution might be lower than the overall standard deviation of the entire image due to the averaging effect. Some blocks might have very low variability, pulling the mean of the block standard deviations down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4f24f-712e-4fed-84c9-6adb3ae505a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eebc97-6737-4913-afd4-83c007dd3997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce96bc-c62f-4f1d-9288-0518686c796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Applying levelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830f5d2-3ca2-438a-9dd7-a3a45fd4a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from x3p import X3Pfile\n",
    "import statsmodels.api as sm  # Required for plane fitting\n",
    "\n",
    "\n",
    "def process_file_with_leveling(filename, block_size=100, outlier_threshold=1.96, frac_sample=0.05):\n",
    "    # Load the .x3p file\n",
    "    x3p_obj = X3Pfile(filename)\n",
    "    \n",
    "    # Extract data and convert to numpy array\n",
    "    data = np.array(x3p_obj.data, dtype='float64')\n",
    "\n",
    "    # Apply leveling\n",
    "    length = data.shape[0]\n",
    "    x, y = np.arange(length), np.arange(length)\n",
    "    xy = sm.add_constant(np.array(np.meshgrid(x, y)).T.reshape(-1, 2))  # Add a constant for fitting\n",
    "    z_xy = data.flatten()\n",
    "    prediction, parameters = plane_leveling(xy, z_xy, frac_sample)\n",
    "    data_leveled = (z_xy - prediction).reshape(length, length)\n",
    "\n",
    "    # Create DataFrame from leveled data\n",
    "    df_leveled = pd.DataFrame(data=data_leveled)\n",
    "    \n",
    "    # Compute block statistics for leveled data\n",
    "    block_stats = compute_block_stats(df_leveled, block_size)\n",
    "    \n",
    "    # Remove outlier pixels from the leveled DataFrame\n",
    "    df_no_outliers = remove_outlier_pixels(df_leveled, block_size, block_stats, outlier_threshold)\n",
    "    \n",
    "    return data_leveled, df_no_outliers.to_numpy(), block_stats\n",
    "\n",
    "def plot_histogram_from_processed_data(processed_data, block_size, title_suffix=''):\n",
    "    # Convert the numpy array to a DataFrame for ease of manipulation\n",
    "    df_processed = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Compute block statistics from the processed DataFrame\n",
    "    block_stats = compute_block_stats(df_processed, block_size)\n",
    "    \n",
    "    # Extract standard deviations from the block_stats dictionary\n",
    "    std_devs = [stats[1] for stats in block_stats.values()]\n",
    "    \n",
    "    # Plotting the histogram of standard deviations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(std_devs, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title(f'Histogram of Standard Deviations for Each Block {title_suffix}')\n",
    "    plt.xlabel('Standard Deviation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "filenames = [\n",
    "    'STV1 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV2 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV3 Rib 1 Surf 8 subset-t.x3p',\n",
    "    'STV4 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV5 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV6 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV7 Rib 8 Surf 2 subset-t.x3p',\n",
    "    'STV8 Rib 4 Surf 8 subset-t.x3p',\n",
    "    'STV9 Rib 1 Surf 8 subset-t.x3p'\n",
    "]\n",
    "\n",
    "block_sizes = [25, 50, 75]  # Different block sizes to try\n",
    "\n",
    "for block_size in block_sizes:\n",
    "    print(f\"Processing with block size: {block_size}x{block_size}\")\n",
    "    for filename in filenames:\n",
    "        leveled_data, processed_data, block_stats = process_file_with_leveling(filename, block_size=block_size)\n",
    "        title_suffix = f\"{filename} - Block Size {block_size}x{block_size}\"\n",
    "        # Plotting data and histograms\n",
    "        plot_data(leveled_data, processed_data, title_suffix=title_suffix)\n",
    "        plot_histogram_from_processed_data(processed_data, block_size, title_suffix=title_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97904ef4-91f2-4351-b47a-637af783ece2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087205e-93b0-4186-b815-e35719202fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad73dbc-ab64-4988-b645-195c38d20299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_calculate_outlier_percentage(original_data, processed_data):\n",
    "    # Calculate the total number of data points\n",
    "    total_data_points = np.prod(original_data.shape)\n",
    "    \n",
    "    # Calculate the number of outliers as the number of NaN values in processed_data\n",
    "    num_outliers = np.isnan(processed_data).sum()\n",
    "    \n",
    "    # Calculate the percentage of outliers removed\n",
    "    percentage_outliers_removed = (num_outliers / total_data_points) * 100\n",
    "    \n",
    "    # Create a mask of the same shape as the original data, initialized to False\n",
    "    outlier_mask = np.zeros_like(original_data, dtype=bool)\n",
    "    \n",
    "    # Mark the positions where data points were removed as outliers\n",
    "    outlier_mask[np.isnan(processed_data)] = True\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(outlier_mask, cmap='hot', interpolation='nearest')\n",
    "    plt.title(\"Outlier Visualization\")\n",
    "    plt.colorbar(label='Outlier Presence')\n",
    "    plt.show()\n",
    "\n",
    "    return percentage_outliers_removed\n",
    "\n",
    "# Assuming 'filenames' is a list of your file names\n",
    "for filename in filenames:\n",
    "    original_data, processed_data, _ = process_file_with_leveling(filename, block_size=100)\n",
    "    percentage_outliers_removed = visualize_and_calculate_outlier_percentage(original_data, processed_data)\n",
    "    print(f\"Filename: {filename}, Percentage of Outliers Removed: {percentage_outliers_removed:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3819f-429b-4c0d-8e3d-f3e3fa10ff18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9418454-e42b-4fb8-88e4-06f4e5a206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "def calculate_ra(data):\n",
    "    \"\"\"\n",
    "    Calculate the Average Roughness (Ra), ignoring NaN values.\n",
    "    \"\"\"\n",
    "    return np.nanmean(np.abs(data - np.nanmean(data)))\n",
    "\n",
    "def calculate_rq(data):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Square Roughness (Rq), ignoring NaN values.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.nanmean(np.square(data - np.nanmean(data))))\n",
    "\n",
    "def calculate_rz(data):\n",
    "    \"\"\"\n",
    "    Calculate the Maximum Height of the Profile (Rz), ignoring NaN values.\n",
    "    \"\"\"\n",
    "    return np.nanmax(data) - np.nanmin(data)\n",
    "\n",
    "def calculate_rsk(data):\n",
    "    \"\"\"\n",
    "    Calculate the Skewness (Rsk) of the surface profile, ignoring NaN values.\n",
    "    \"\"\"\n",
    "    return skew(data[~np.isnan(data)], bias=False)\n",
    "\n",
    "def calculate_rku(data):\n",
    "    \"\"\"\n",
    "    Calculate the Kurtosis (Rku) of the surface profile, ignoring NaN values.\n",
    "    \"\"\"\n",
    "    return kurtosis(data[~np.isnan(data)], fisher=False, bias=False)\n",
    "\n",
    "\n",
    "ra = calculate_ra(data_no_outliers)\n",
    "rq = calculate_rq(data_no_outliers)\n",
    "rz = calculate_rz(data_no_outliers)\n",
    "rsk = calculate_rsk(data_no_outliers)\n",
    "rku = calculate_rku(data_no_outliers)\n",
    "\n",
    "print(f\"Average Roughness (Ra): {ra}\")\n",
    "print(f\"Root Mean Square Roughness (Rq): {rq}\")\n",
    "print(f\"Maximum Height of the Profile (Rz): {rz}\")\n",
    "print(f\"Skewness (Rsk): {rsk}\")\n",
    "print(f\"Kurtosis (Rku): {rku}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb29d4-59fd-44ac-911f-d722ee456963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "from x3p import X3Pfile\n",
    "\n",
    "# Your statistical measure calculation functions remain the same\n",
    "\n",
    "def process_and_calculate_stats(filename, block_size=100, outlier_threshold=1.96):\n",
    "    # Load the .x3p file\n",
    "    x3p_obj = X3Pfile(filename)\n",
    "    \n",
    "    # Extract data and convert to numpy array\n",
    "    data = np.array(x3p_obj.data, dtype='float64')\n",
    "    \n",
    "    # Create DataFrame from original data\n",
    "    df = pd.DataFrame(data=data)\n",
    "    \n",
    "    # Compute block statistics\n",
    "    block_stats = compute_block_stats(df, block_size)\n",
    "    \n",
    "    # Remove outlier pixels from the DataFrame\n",
    "    df_no_outliers = remove_outlier_pixels(df, block_size, block_stats, outlier_threshold)\n",
    "    \n",
    "    # Convert the modified DataFrame back to a NumPy array for statistical calculations\n",
    "    data_no_outliers = df_no_outliers.to_numpy()\n",
    "\n",
    "    # Calculate statistics\n",
    "    ra = calculate_ra(data_no_outliers)\n",
    "    rq = calculate_rq(data_no_outliers)\n",
    "    rz = calculate_rz(data_no_outliers)\n",
    "    rsk = calculate_rsk(data_no_outliers)\n",
    "    rku = calculate_rku(data_no_outliers)\n",
    "\n",
    "    return ra, rq, rz, rsk, rku\n",
    "\n",
    "filenames = [\n",
    "    'STV1 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV2 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV3 Rib 1 Surf 8 subset-t.x3p',\n",
    "    'STV4 Rib 3 Surf 8 subset-t.x3p',\n",
    "    'STV5 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV6 Rib 5 Surf 8 subset-t.x3p',\n",
    "    'STV7 Rib 8 Surf 2 subset-t.x3p',\n",
    "    'STV8 Rib 4 Surf 8 subset-t.x3p',\n",
    "    'STV9 Rib 1 Surf 8 subset-t.x3p'\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    ra, rq, rz, rsk, rku = process_and_calculate_stats(filename)\n",
    "    print(f\"File: {filename}\")\n",
    "    print(f\"Average Roughness (Ra): {ra}\")\n",
    "    print(f\"Root Mean Square Roughness (Rq): {rq}\")\n",
    "    print(f\"Maximum Height of the Profile (Rz): {rz}\")\n",
    "    print(f\"Skewness (Rsk): {rsk}\")\n",
    "    print(f\"Kurtosis (Rku): {rku}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6b92d-cdf9-4f21-88af-dbc41a10d7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab7ff4-17a9-47d5-9fa2-fc50e5408fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87359964-5590-47d5-8d7b-494d7f8b15de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fbce5-eeca-4e58-ad96-250e46d3009c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36597310-b40b-46f3-a39e-10cebc8a3bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62789b54-1343-40f6-b9dd-6b8d295b1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "\n",
    "# Function to compute roughness parameters for each block\n",
    "def compute_block_roughness(df, block_size=100):\n",
    "    nrows, ncols = df.shape\n",
    "    block_roughness_stats = []\n",
    "    for i in range(0, nrows, block_size):\n",
    "        for j in range(0, ncols, block_size):\n",
    "            block = df.iloc[i:i+block_size, j:j+block_size].to_numpy()\n",
    "            if np.isnan(block).all():\n",
    "                continue  # Skip blocks that are entirely NaN\n",
    "            ra = calculate_ra(block)\n",
    "            rq = calculate_rq(block)\n",
    "            rz = calculate_rz(block)\n",
    "            rsk = calculate_rsk(block)\n",
    "            rku = calculate_rku(block)\n",
    "            block_roughness_stats.append(((i // block_size, j // block_size), ra, rq, rz, rsk, rku))\n",
    "    return block_roughness_stats\n",
    "\n",
    "# Compute roughness parameters for each block\n",
    "block_roughness_stats = compute_block_roughness(df_no_outliers, 100)\n",
    "\n",
    "# Initialize plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 14))\n",
    "fig.suptitle('Surface Roughness Metrics Across Blocks', fontsize=16)\n",
    "\n",
    "# Titles for each subplot\n",
    "titles = ['Average Roughness (Ra)', 'Root Mean Square Roughness (Rq)', \n",
    "          'Maximum Height of the Profile (Rz)', 'Skewness (Rsk)', 'Kurtosis (Rku)']\n",
    "\n",
    "# Function to plot each metric\n",
    "def plot_metric(ax, values, title):\n",
    "    sc = ax.scatter(x_positions, y_positions, c=values, cmap='viridis', s=100)\n",
    "    fig.colorbar(sc, ax=ax, orientation='vertical').set_label(title)\n",
    "    ax.set_xlabel('Block X Position')\n",
    "    ax.set_ylabel('Block Y Position')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Extract positions and metrics\n",
    "x_positions, y_positions = zip(*[stats[0] for stats in block_roughness_stats])\n",
    "metrics = zip(*[stats[1:] for stats in block_roughness_stats])\n",
    "\n",
    "# Plot each metric\n",
    "for ax, values, title in zip(axs.flat, metrics, titles):\n",
    "    plot_metric(ax, values, title)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54611aaa-331c-48bb-aac3-c8fa356435d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50eb268-27bb-450e-9b20-8b9187c094a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3209d-9928-4145-89bf-a1bb2c114fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
